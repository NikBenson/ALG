\section{Mathematische Eigenschaften}
	Wie die meisten Algorithmen ist Samplesort mathematisch darstellbar.
	Hierbei gibt es viele Perspektiven.
	Im Folgenden ist die Laufzeit- und Speicherkomplexität dargestellt.
	
	\subsection{Speicherkomplexität}
		Samplesort ist ein in-place Sortieralgorithmus.
		Das heißt, es wird kein zusätzlicher Speicher benötigt.\\
		Soweit die Theorie zur Standard Variante.
		Anders ist es beim Super Scalar Sample Sort (sss-Sort). \autocite{sanders-2004} 
		Dieser benötigt einen 2. temporären Array mit n Elementen und damit doppelt so viel Speicher.
		An der Speicherkomplexität ändert sich damit allerdings nichts, diese bleibt bei $\Theta(n)$.\\
		Jedoch sind auch andere Möglichkeiten denkbar, die Speicher nutzen, um die Laufzeit zu begünstigen.
		Im Regelfall ist dies allerdings nicht von Vorteil, da große Datenmengen meist nicht im RAM alleine gehandhabt werden können.
		Also müssen die Daten konstant zwischen virtuellem RAM, also Speicher auf einer Festplatte, und dem Hardware RAM hin und her bewegt werden.
		Diesen Vorgang gilt es in jedem Fall zu vermeiden.

	\subsection{Laufzeitkomplexität}
		Ein häufig verwendetes Maß für die Performanz eines Sortieralgorithmus sind die benötigten vergleiche.
		Diese Differenzierung ist wichtig, da verschiedene Operationen\footnote{Im Regelfall Vergleichsoperationen und Speicherzugriffe}, bei unterschiedlichen Datenmodellen, unterschiedlich gewichtet Zeit, bzw. Prozessoroperationen, benötigen.
		Wenn, wie in \ref{sec:implementation}, lediglich Integer verglichen werden\footnote{In der Regel benötigt diese Operation lediglich so viel Zeit, wie eine Subtraktionsoperation}, so ist die benötigte Zeit vernachlässigbar.
		Allerdings tritt ein Problem mit den sogenannten Conditional branches, also den bedingten Abzweigungen der Byte~code~instruktionen, auf:
		Werden die Anweisungen in das Register, also den Anweisungscache, des Prozessors geladen, so ist die Wahrscheinlichkeit für jede Abzweigung, also jedes Ergebnis der Vergleichsoperation, bei $n\log{n}$\footnote{Da jedes Element transitiv mit jedem anderen verglichen wird} vergleichen, annähernd $50\%$.
		Damit kann keine sinnvolle Annahme getroffen werden, welche Instruktionen in das Register geladen werden sollen.
		Wird der falsche Zweig genommen, so muss die Operation wiederholt werden und wenn alle Möglichkeiten geladen werden, wird für jeden Abzweig die Last verdoppelt. \autocite{sanders-2004}\\
		Damit ist es dennoch in unserem Interesse, die Anzahl der vergleiche gering zu halten, außer dieses Problem wird, mit der Parallelisierung der Vergleiche, durch sss-Sort umgangen.
		
		\paragraph{Sortieren der Sample}
			Beim Sortieren der Sample werden $C_\textit{smallSort}(s)$ vergleiche benötigt, wobei $C_\textit{smallSort}(n)$ die Funktion der Vergleiche von \textit{smallSort} ist, und $s$ die Größe der Sample.
			Für Quicksort gilt durchschnittlich $C_\textit{smallSort}(n)=n\log{n}$. \autocite{wikipedia-contributors-2022A}
			
		\paragraph{Schwellenwertunterschreitung}
			Ist die durchschnittliche Größe eines Buckets kleiner als der Schwellenwert, also $\frac{n}{s+1}\leq \textit{threshold}$, so gilt erneut $C_\textit{threshold}(n)\equiv C_\textit{smallSort}=n\log{n}$
		
		\paragraph{Verteilen auf Buckets}
			Das Verteilen auf die Buckets ist an sich bereits ein Divide-And-Conquer-Algorithmus.
			Seien $s$ die Anzahl der Splitter und $n$ die Anzahl der Elemente, die keine Splitter sind, so lässt sich folgende Rekursionsgleichung\footnote{Auch bekannt als Differenzialgleichung} ableiten, vorausgesetzt $s,n\in \left\{ \mathbb{N}\to 2^n \right\}$, also $s$ und $n$ Potenzen von 2 sind:
			\begin{equation}
				C_{s,n}=n+2\cdot C_{\frac{s-1}{2}, \frac{n}{2}}\ \text{für $s>1$ mit}\ C_{1,n}=n
			\end{equation}
			Über die folgende OGF\footnote{Ordinary Generating Function, also eine Sigmafunktion oder Summenformel, die eine Funktion generiert}, lässt sich leicht die Anzahl von $s\cdot n$ ableiten:
			\begin{equation}
				C_s(n)=\sum_{k=1}^{s}\left(2^{k-1}\cdot \frac{n}{2^{k-1}}\right)=\sum_{k=1}^{s}n
			\end{equation}
		\paragraph
			Zusammengesetzt ergibt sich folgende Funktion, für eine einfache Implementierung, ohne die Splitter aus den Buckets zu nehmen und $n>\textit{threshold}$:
			\begin{equation}
				\label{equ:comparisons}
				\begin{aligned}
					C_{n}&=C_\textit{smallSort}\left(s\cdot \textit{oversamplingFactor}\right)\ +\ ns\ +\ (s+1)\cdot C_{\frac{n}{s+1}}\ \text{für $\frac{n}{s} \geq \textit{threshold}$}\\
					\text{mit}\ C_{n}&=C_\textit{threshold}(n)\ \text{für $\frac{n}{s} < \textit{threshold}$}\\
					&\Leftrightarrow \sum_{k=1}^{\log_{s+1}\left(\frac{n}{\textit{threshold}}\right)}\left((s+1)^{k-1}\cdot\left(C_{\textit{smallSort}}(s\cdot\textit{oversamplingFactor})+\frac{ns}{(s+1)^{k-1}}\right)\right)\\
					&\>\>\> +\log_{s+1}\left(\frac{n}{\textit{threshold}}\right)\cdot C_{\textit{threshold}}\left(\frac{n}{\log_{s+1}\left(\frac{n}{\textit{threshold}}\right)}\right)\\
					&\equiv C_{\textit{smallSort}}(s\cdot\textit{oversamplingFactor})\cdot\sum_{k=1}^{\log_{s+1}\left(\frac{n}{\textit{threshold}}\right)}(s+1)^{k-1}\\
					&\>\>\> +\log_{s+1}\left(\frac{n}{\textit{threshold}}\right)\cdot \left(ns+C_{\textit{threshold}}\left(\frac{n}{\log_{s+1}\left(\frac{n}{\textit{threshold}}\right)}\right)\right)\\
					&\equiv C_{\textit{smallSort}}(s\cdot\textit{oversamplingFactor})\cdot\frac{(s+1)^{\log_{s+1}\left(\frac{n}{\textit{threshold}}\right)}-1}{s}\\
					&\>\>\> +\log_{s+1}\left(\frac{n}{\textit{threshold}}\right)\cdot \left(ns+C_{\textit{threshold}}\left(\frac{n}{\log_{s+1}\left(\frac{n}{\textit{threshold}}\right)}\right)\right)\\
					&=C_{\textit{smallSort}}(s\cdot\textit{oversamplingFactor})\cdot\frac{n-\textit{threshold}}{\textit{threshold}\cdot s}\\
					&\>\>\> +\log_{s+1}\left(\frac{n}{\textit{threshold}}\right)\cdot \left(ns+C_{\textit{threshold}}\left(\frac{n}{\log_{s+1}\left(\frac{n}{\textit{threshold}}\right)}\right)\right)
				\end{aligned}
			\end{equation}
			Setzen wir für $C_{\textit{threshold}}(n)=C_{\textit{smallSort}}(n)=n\log_2{n}$, $\textit{oversamplingFactor}=128$, $\textit{threshold}=256$ und $s=15$ ein, so folgt daraus
			\begin{equation}
				\begin{aligned}
					C_n&=15\cdot 128\cdot\log_2\left(15\cdot 128\right)\cdot \frac{n-256}{256\cdot15}+\log_{16}\left(\frac{n}{256}\right)\cdot\left(15n+\frac{n}{\log_{16}\left(\frac{n}{256}\right)}\log_2\left(\frac{n}{\log_{16}\left(\frac{n}{256}\right)}\right)\right)\\
					&\equiv 15n\log_{16}\left(\frac{n}{256}\right)+n\log_{2}\left(\frac{n}{\log_{16}\left(\frac{n}{256}\right)}\right)+\frac{n}{2}\log_2\left(1920\right)-128\log_2\left(1920\right)
				\end{aligned}
			\end{equation}
			Dies ist deutlich näher an $\log_2{n!}$ als, z.B. Quicksort mit $n\log_2{n}$. 
			Und da die Vergleiche, zu annähernd gleichen Teilen, auf alle Prozessoren verteilt werden, können wir durch dessen Anzahl teilen.
			
		\paragraph{Big O}
			Da nicht nur Vergleiche durchgeführt werden, weicht die tatsächliche Laufzeit etwas von den Vergleichen ab.
			Zum Finden der Splitter ist dies $O\left(\frac{n}{p}\log{p}\right)$, für das Verteilen auf die Buckets untergliedert es sich in: \autocite{wikipedia-contributors-2021}
			\begin{itemize}
				\item $O(p)$ zum Lesen aller Knoten
				\item $O(\log{p})$ für das Broadcasten (Managen des asynchronen Prozesses)
				\item $O\left(\frac{n}{p}\log{p}\right)$, um binär nach allen Elementen zu suchen
				\item $O\left(\frac{n}{p}\right)$ zum Senden der Elemente zum entsprechenden Prozessor
			\end{itemize}
			Und zu guter letzt das Sortieren der Buckets, beispielsweise mit Quicksort, also $O\left(\frac{n}{p}\log{\frac{n}{p}}\right)$.\\
			Insgesamt ist das $O\left(\frac{n}{p}\log{\frac{n}{p}}\right)$, da $O$ den konstanten Faktor der ersten Iteration vernachlässigt.\\
			Zu beachten ist, dass in \ref{equ:comparisons} eine Rekursive Implementierung dargestellt ist, und in diesem Abschnitt Quicksort als sekundärer Algorithmus verwendet wird.
			Dies macht allerdings keinen Unterschied aus, da Big O der beiden Algorithmen identisch ist.
			Der wichtige unterschied, den Big O hier verbirgt ist, dass
			\begin{enumerate}
				\item Quicksort bei entarteten Daten eine Laufzeit von $O\left(n^2\right)$ hat.
					Dies ist bei Samplesort nur bei einigen Implementierungen der Fall, wenn alle Elemente identisch sind.
					Hier können Equality Buckets Abhilfe schaffen.
				\item das teilen durch $p$ eine Vereinfachung ist, da nicht jeder Bucket gleich groß ist.
					Allerdings ist die Standardabweichung, zum einen, abgesicherter gegen entartete Daten, und zum anderen deutlich geringer durch das Sampling.
			\end{enumerate}
			
